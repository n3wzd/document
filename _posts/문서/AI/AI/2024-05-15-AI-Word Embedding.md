---
categories:
- AI
date: '2024-05-15'
title: '[AI] Word Embedding'
---

{% raw %}
단어 임베딩(Word Embedding)은 자연어 처리(NLP)에서 단어를 수치 벡터로 표현하는 기술입니다. 단어 임베딩을 통해 단어를 고정 길이의 실수 벡터로 변환할 수 있으며, 이러한 벡터는 단어의 의미적 유사성을 반영합니다. 즉, 의미가 비슷한 단어들은 임베딩 벡터 공간에서 서로 가깝게 위치하게 됩니다.

1. **벡터 표현**:
   - 단어를 고정 길이의 실수 벡터로 변환합니다.
   - 벡터의 각 차원은 단어의 특정 의미적 특성을 나타냅니다.
2. **의미적 유사성**:
   - 임베딩 벡터 공간에서 의미가 유사한 단어들은 가까운 거리에 위치하게 됩니다.
   - 예를 들어, "king"과 "queen", "apple"과 "fruit"은 각각 임베딩 벡터 공간에서 서로 가까운 위치에 있을 것입니다.
3. **학습 방법**:
   - 단어 임베딩은 주로 대규모 텍스트 데이터에서 학습됩니다.
   - 대표적인 방법으로 Word2Vec, GloVe, FastText 등이 있습니다.

## Word2Vec
Word2Vec은 구글에서 개발한 단어 임베딩 기법으로, 단어를 벡터 공간에 임베딩하여 의미적으로 유사한 단어들이 가까운 거리에 위치하도록 합니다.

Word2Vec에서 주로 사용되는 모델 종류는 다음과 같습니다:
1. **CBOW (Continuous Bag of Words)**:
   - CBOW 모델은 주변 단어들(Context words)을 통해 중심 단어(Target word)를 예측하는 방식입니다.
   - 예를 들어, 문장에서 "The cat sits on the mat"가 있을 때, "The", "cat", "on", "the", "mat" 같은 주변 단어들을 사용하여 "sits"라는 중심 단어를 예측합니다.
   - CBOW 모델은 문맥의 평균적인 표현을 통해 중심 단어를 예측하므로, 데이터셋이 클 때 효율적으로 학습할 수 있습니다.
2. **Skip-gram**:
   - Skip-gram 모델은 중심 단어(Target word)를 통해 주변 단어들(Context words)을 예측하는 방식입니다.
   - 같은 예시에서, 중심 단어 "sits"를 사용하여 주변 단어들 "The", "cat", "on", "the", "mat"를 예측합니다.
   - Skip-gram 모델은 희귀한 단어에 대한 표현을 학습하는 데 더 효과적입니다.

Word2Vec의 학습 과정은 다음과 같습니다:

1. **단어 벡터 초기화**:
   - 각 단어는 임의의 작은 값으로 초기화된 고정 길이의 벡터로 표현됩니다.
2. **단어 예측**:
   - CBOW 모델에서는 주변 단어들을 평균하여 중심 단어를 예측합니다.
   - Skip-gram 모델에서는 중심 단어를 사용하여 주변 단어들을 예측합니다.
3. **손실 함수 계산**:
   - 모델이 예측한 단어와 실제 단어 사이의 차이를 계산하여 손실을 구합니다.
   - 일반적으로 음성 샘플링(Negative Sampling) 또는 계층적 소프트맥스(Hierarchical Softmax) 같은 방법을 사용하여 계산 효율성을 높입니다.
4. **역전파를 통해 가중치 업데이트**:
   - 손실을 최소화하기 위해 역전파 알고리즘을 사용하여 단어 벡터를 업데이트합니다.
   - 이 과정은 반복적으로 수행되어 단어 벡터가 최적화됩니다.
{% endraw %}